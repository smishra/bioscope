/*
    MapReduce library

    Copyright (C) 2009 Craig Henderson.
    cdm.henderson@gmail.com

    Use, modification and distribution is subject to the
    Boost Software License, Version 1.0 - August 17th, 2003

    Permission is hereby granted, free of charge, to any person or organization
    obtaining a copy of the software and accompanying documentation covered by
    this license (the "Software") to use, reproduce, display, distribute,
    execute, and transmit the Software, and to prepare derivative works of the
    Software, and to permit third-parties to whom the Software is furnished to
    do so, all subject to the following:

    The copyright notices in the Software and this entire statement, including
    the above license grant, this restriction and the following disclaimer,
    must be included in all copies of the Software, in whole or in part, and
    all derivative works of the Software, unless such copies or derivative
    works are solely in the form of machine-executable object code generated by
    a source language processor.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
    SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
    FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
    ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.
*/

#ifndef MAPREDUCE_CPU_PARALLEL_HPP
#define MAPREDUCE_CPU_PARALLEL_HPP

#include <boost/thread.hpp>

namespace mapreduce {

namespace schedule_policy {

namespace detail {

template<typename Job>
inline void run_next_map_task(Job &job, boost::mutex &m1, boost::mutex &m2, results &result)
{
    try
    {
        bool run = true;
        while (run)
        {
            void *key = 0;

            m1.lock();
            run = job.get_next_map_key(key);
            m1.unlock();

            if (run)
                job.run_map_task(key, result, m2);
        }
    }
    catch (std::exception &e)
    {
        std::cerr << "\nError: " << e.what() << "\n";
    }
}

template<typename Job>
inline void run_next_reduce_task(Job &job, unsigned &partition, boost::mutex &mutex, results &result)
{
    try
    {
        while (1)
        {
            boost::mutex::scoped_lock l(mutex);
            unsigned part = partition++;
            if (part < job.number_of_partitions())
            {
                l.unlock();
                job.run_reduce_task(part, result);
            }
            else
                break;
        }
    }
    catch (std::exception &e)
    {
        std::cerr << "\nError: " << e.what() << "\n";
    }
}

template<typename Job>
void run_intermediate_results_shuffle(Job &job, unsigned const partition, results &result)
{
    try
    {
        using namespace boost::posix_time;
        ptime start_time(microsec_clock::universal_time());
        job.run_intermediate_results_shuffle(partition);
        result.shuffle_times.push_back(microsec_clock::universal_time()-start_time);
    }
    catch (std::exception &e)
    {
        std::cerr << "\nError: " << e.what() << "\n";
    }
}

}   // namespace detail


template<typename Job>
class cpu_parallel
{
  public:
    void operator()(Job &job, results &result)
    {
        unsigned const num_cpus = std::max(1U,boost::thread::hardware_concurrency());

        typedef std::vector<boost::shared_ptr<results> > all_results_t;
        all_results_t all_results;
        boost::mutex  m1, m2;

        // run the Map Tasks
        using namespace boost::posix_time;
        ptime start_time(microsec_clock::universal_time());

        unsigned const map_tasks = std::max(num_cpus,std::min(num_cpus, job.number_of_map_tasks()));

        boost::thread_group map_threads;
        for (unsigned loop=0; loop<map_tasks; ++loop)
        {
            boost::shared_ptr<results> this_result(new results);
            all_results.push_back(this_result);

            boost::thread *thread =
                new boost::thread(
                    detail::run_next_map_task<Job>,
                    boost::ref(job),
                    boost::ref(m1),
                    boost::ref(m2),
                    boost::ref(*this_result));
            map_threads.add_thread(thread);
        }
        map_threads.join_all();
        result.map_runtime = microsec_clock::universal_time() - start_time;

        // Intermediate results shuffle
        boost::thread_group shuffle_threads;
        start_time = microsec_clock::universal_time();
        for (unsigned partition=0; partition<job.number_of_partitions(); ++partition)
        {
            for (unsigned loop=0;
                 loop<num_cpus  &&  partition<job.number_of_partitions();
                 ++loop, ++partition)
            {
                boost::shared_ptr<results> this_result(new results);
                all_results.push_back(this_result);

                boost::thread *thread =
                    new boost::thread(
                        detail::run_intermediate_results_shuffle<Job>,
                        boost::ref(job),
                        partition,
                        boost::ref(*this_result));
                shuffle_threads.add_thread(thread);

            }
        }
        shuffle_threads.join_all();
        result.shuffle_runtime = microsec_clock::universal_time() - start_time;

        // run the Reduce Tasks
        boost::thread_group reduce_threads;
        unsigned const reduce_tasks =
            std::min<unsigned const>(num_cpus, job.number_of_partitions());

        unsigned partition = 0;
        start_time = microsec_clock::universal_time();
        for (unsigned loop=0; loop<reduce_tasks; ++loop)
        {
            boost::shared_ptr<results> this_result(new results);
            all_results.push_back(this_result);

            boost::thread *thread =
                new boost::thread(
                    detail::run_next_reduce_task<Job>,
                    boost::ref(job),
                    boost::ref(partition),
                    boost::ref(m1),
                    boost::ref(*this_result));
            reduce_threads.add_thread(thread);
        }
        reduce_threads.join_all();
        result.reduce_runtime = microsec_clock::universal_time() - start_time;

        // we're done with the map/reduce job, collate the statistics before returning
        for (all_results_t::const_iterator it=all_results.begin();
             it!=all_results.end();
             ++it)
        {
            result.counters.map_keys_executed     += (*it)->counters.map_keys_executed;
            result.counters.map_key_errors        += (*it)->counters.map_key_errors;
            result.counters.map_keys_completed    += (*it)->counters.map_keys_completed;
            result.counters.reduce_keys_executed  += (*it)->counters.reduce_keys_executed;
            result.counters.reduce_key_errors     += (*it)->counters.reduce_key_errors;
            result.counters.reduce_keys_completed += (*it)->counters.reduce_keys_completed;

            std::copy(
                (*it)->map_times.begin(),
                (*it)->map_times.end(),
                std::back_inserter(result.map_times));
            std::copy(
                (*it)->shuffle_times.begin(),
                (*it)->shuffle_times.end(),
                std::back_inserter(result.shuffle_times));
            std::copy(
                (*it)->reduce_times.begin(),
                (*it)->reduce_times.end(),
                std::back_inserter(result.reduce_times));

        }
        result.counters.actual_map_tasks    = map_tasks;
        result.counters.actual_reduce_tasks = reduce_tasks;
        result.counters.num_result_files    = job.number_of_partitions();
    }
};

}   // namespace schedule_policy

}   // namespace mapreduce 

#endif  // MAPREDUCE_CPU_PARALLEL_HPP
